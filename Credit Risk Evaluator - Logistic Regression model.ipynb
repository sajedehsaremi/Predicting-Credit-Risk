{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdc6ce51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91449bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(Path('Resources/2019loans.csv'))\n",
    "test_df = pd.read_csv(Path('Resources/2020Q1loans.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecbdbab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12180, 86)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7265b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4702, 86)\n"
     ]
    }
   ],
   "source": [
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "443622c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12180, 86)\n"
     ]
    }
   ],
   "source": [
    "# Convert categorical data to numeric and separate target feature for training data\n",
    "\n",
    "# Get X data by removing loan_status column (outcome)\n",
    "X = train_df.drop(['loan_status', 'Unnamed: 0'], axis=1)\n",
    "X_dummies = pd.get_dummies(X, drop_first=True)\n",
    "print(X_dummies.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "598e7667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4702, 85)\n",
      "debt_settlement_flag_Y\n"
     ]
    }
   ],
   "source": [
    "# Convert categorical data to numeric and separate target feature for testing data\n",
    "X_test = test_df.drop(['loan_status', 'Unnamed: 0'], axis=1)\n",
    "X_test_dummies = pd.get_dummies(X_test, drop_first=True)\n",
    "print(X_test_dummies.shape)\n",
    "\n",
    "for el in X_dummies.columns:\n",
    "    if el not in X_test_dummies.columns:\n",
    "        print(el)\n",
    "\n",
    "# Add missing column to test dummies df\n",
    "X_test_dummies['debt_settlement_flag_Y'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3290ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert output labels to 0 and 1\n",
    "y_label = LabelEncoder().fit_transform(train_df['loan_status'])\n",
    "y_test_label = LabelEncoder().fit_transform(test_df['loan_status'])\n",
    "\n",
    "# 1 = low risk, 0 = high risk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee922217",
   "metadata": {},
   "source": [
    "I anticipate that the Logistic Regression model will perform better than the Random Forest Classifier because the latter does not tend to generalize well and we are working with a very large number of features here. I believe the Random Forest Classifier is more likely to overfit to noise in the training data and thus perform poorly with the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb64741e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sajedeh\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Logistic Regression model on the unscaled data and print the model score\n",
    "logmodel = LogisticRegression()\n",
    "logmodel.fit(X_dummies, y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5048d604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Score: 0.6553366174055829\n",
      "Testing Data Score: 0.5202041684389621\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training Data Score: {logmodel.score(X_dummies, y_label)}\")\n",
    "print(f\"Testing Data Score: {logmodel.score(X_test_dummies, y_test_label)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c926e4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 1.0\n",
      "Testing Score: 0.6707783921735432\n"
     ]
    }
   ],
   "source": [
    "# Train a Random Forest Classifier model and print the model score\n",
    "clf = RandomForestClassifier(random_state=1, n_estimators=50).fit(X_dummies, y_label)\n",
    "print(f'Training Score: {clf.score(X_dummies, y_label)}')\n",
    "print(f'Testing Score: {clf.score(X_test_dummies, y_test_label)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8b9570",
   "metadata": {},
   "source": [
    "I believe that the after scaling the data to remove bias from features with higher magnitudes, the Random Forest Classifier will perform better than the Logistic Regression model both in the training set and in the testing set, since the Random Forest model is designed for more complex relationships between features and doesn't assume linearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f9e83ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "scaler = StandardScaler().fit(X_dummies)\n",
    "X_train_scaled = scaler.transform(X_dummies)\n",
    "X_test_scaled = scaler.transform(X_test_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa9e61de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 1.0\n",
      "Testing Score: 0.6714164185452999\n"
     ]
    }
   ],
   "source": [
    "# Train a Random Forest Classifier model on the scaled data and print the model score\n",
    "clf1 = RandomForestClassifier(random_state=1, n_estimators=50).fit(X_train_scaled, y_label)\n",
    "print(f'Training Score: {clf1.score(X_train_scaled, y_label)}')\n",
    "print(f'Testing Score: {clf1.score(X_test_scaled, y_test_label)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b522b19",
   "metadata": {},
   "source": [
    "### Log Model scores:\n",
    "Unscaled data: \n",
    "\n",
    "    Training Data Score: 0.6575533661740558\n",
    "    Testing Data Score: 0.5204168438962143\n",
    "    \n",
    "Scaled data:\n",
    "\n",
    "    Training Score: 0.7130541871921182\n",
    "    Testing Score: 0.7216078264568269\n",
    "\n",
    "Random Forest scores: Unscaled data: Training Score: 1.0 Testing Score: 0.6707783921735432 Scaled data: Training Score: 1.0 Testing Score: 0.6714164185452999\n",
    "\n",
    "Scaling the data had a very good effect on the outcome of the Log model. The scores for both the training and test sets increased, and the scores got closer together (0.13 difference reduced to 0.01), meaning the Log model is now likely to generalize well too. Scaling the data did not really have much effect on the Random Forest classifier. It still appears to be the case that the model is overfitting on the training data and thus performing poorly on the test data set. Overall, I believe the Log Model (with scaled data) is the better choice in this situation, since it seems to be less sensitive to noise in the data or any collinearity between features. I think we would see a drastic improvement in the Random Forest model if we implemented feature selection techniques, but that might be overkill for this use case. My predicition about the effect scaling would have on the models was not quite accurate. My first prediction, where I mention that random forests do not tend to generalize well, was closer to the truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78d7c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
